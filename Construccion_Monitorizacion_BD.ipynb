{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de la Base de Datos y Monitorización del Incremento de SNPs:\n",
    "\n",
    "1. Generación de paquetes de 500 muestras distribuidas aleatoriamente.\n",
    "\n",
    "2. Filtrado de SNPs y Construcción de la Base de Datos.\n",
    "\n",
    "3. Generación archivo curva de Rarefacción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# Fijar semilla de números aleatorios\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = \"/home/laura/andrea/scripts/TB_DDBB_Building/Paquetes_500_muestras/\"\n",
    "input_directory2 = \"/media/NASII/Datos/ANALYSIS/Proyectos/BBDD_TB/AncestorII/Illumina/Variants\"\n",
    "samples_directory = \"/home/laura/andrea/scripts/TB_DDBB_Building/Muestras\"\n",
    "\n",
    "pack_number = 1\n",
    "\n",
    "df_files = pd.DataFrame(columns=['POS', 'REF', 'ALT', 'COUNT', 'SAMPLES'])\n",
    "\n",
    "SNP_monitoring_df = pd.DataFrame(columns=['PACK', 'num_SNPs'])\n",
    "\n",
    "################ Creación de enlaces simbólicos del paquete de muestras a procesar ################\n",
    "\n",
    "# Obtener nombres del total de muestras analizadas del directorio de almacenamiento\n",
    "folder_names_set = set(os.listdir(input_directory2))\n",
    "\n",
    "# Iterar sobre los archivos indicados en los paquetes que se encuentran en input_directory\n",
    "for pack_file in sorted(os.listdir(input_directory)):\n",
    "\n",
    "    pack_path = os.path.join(input_directory, pack_file)\n",
    "    \n",
    "    # Obtener nombres de las muestras del paquete\n",
    "    with open(pack_path, \"r\") as f:\n",
    "        samples_pack = set(f.read().splitlines())\n",
    "\n",
    "    # Si los nombres de las muestras del paquete (n = 500) se encuentran en la lista de nombres del directorio de almacenamiento (n = 100mil)\n",
    "    for sample_name in samples_pack:\n",
    "        if sample_name in folder_names_set:\n",
    "\n",
    "            # Se busca el archivo tsv en el directorio con el nombre de la muestra\n",
    "            sample_folder = os.path.join(input_directory2, sample_name)\n",
    "            tsv_files = sorted(glob.glob(os.path.join(sample_folder, \"snps.all.ivar.tsv\")))\n",
    "\n",
    "            for tsv_file in tsv_files:\n",
    "\n",
    "                tsv_file_name = f'{sample_name}_snps.all.ivar.tsv'\n",
    "                link_name = os.path.join(samples_directory, tsv_file_name)\n",
    "\n",
    "                # Se crea enlace simbólico a ese archivo tsv en el directorio de salida\n",
    "                if not os.path.exists(link_name): # Evitar errores si ya existe el enlace\n",
    "                    os.symlink(tsv_file, link_name)\n",
    "\n",
    "\n",
    "    print(f\"Los enlaces simbólicos a los archivos de las muestras del paquete {pack_number} ya están disponibles en {samples_directory}.\")\n",
    "\n",
    "    ################ Ejecución del procesamiento de SNPs ################\n",
    "\n",
    "    # Búsqueda de archivos TSV en todas las subcarpetas:\n",
    "    for folder_path, subfolders, files in os.walk(samples_directory):\n",
    "\n",
    "        subfolders.sort()\n",
    "        files.sort()\n",
    "\n",
    "        if os.path.isdir(folder_path):\n",
    "            tsv_files = sorted(glob.glob(os.path.join(folder_path, \"*_snps.all.ivar.tsv\")))  # Busca archivos en cada subcarpeta con el patrón especificado\n",
    "\n",
    "            for file in tsv_files:\n",
    "                try:\n",
    "                    df_file_sample = pd.read_csv(file, sep='\\t')\n",
    "\n",
    "                    sample_name = os.path.basename(file).split(\"_\")[0] # Se queda con el ID de la muestra\n",
    "                    df_file_sample[\"SAMPLES\"] = sample_name\n",
    "\n",
    "                    # Filtro de calidad\n",
    "                    df_file_sample = df_file_sample[(df_file_sample['TOTAL_DP'] >= 20) & (df_file_sample['ALT_FREQ'] >= 0.7)]\n",
    "                            \n",
    "                    # Conserva solo los SNPs (se eliminan INDELs)\n",
    "                    df_file_sample = df_file_sample[(df_file_sample['TYPE'] == \"snp\")]\n",
    "\n",
    "                    # Se eliminan aquellos SNPS que caen en regiones complejas (muchos SNPs marcadores dentro de un intervalo de distancia pequeño)\n",
    "                    df_file_sample = df_file_sample[(df_file_sample['OLDVAR'] != \"complex\")]\n",
    "\n",
    "                    if not df_file_sample.empty:\n",
    "\n",
    "                        # Solo para el primer paquete\n",
    "                        if df_files.empty:\n",
    "                            df_files = df_file_sample\n",
    "\n",
    "                        # Para el resto de paquetes\n",
    "                        else:\n",
    "                            df_files = pd.concat([df_files, df_file_sample], ignore_index = True)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f'Error procesando {file}: {e}')\n",
    "\n",
    "    # Fusionar filas con el mismo POS, REF y ALT:\n",
    "    df_files = df_files.groupby(['POS', 'REF', 'ALT'], as_index=False).agg({'SAMPLES': lambda x: ', '.join(sorted(set(x)))}) # Unir nombres de muestras sin duplicados\n",
    "        \n",
    "    df_files['COUNT'] = df_files['SAMPLES'].apply(lambda x: len(x.split(\", \")))\n",
    "\n",
    "    print(df_files)\n",
    "\n",
    "    ################ Generación del archivo registro de la evolución de la BBDD ################\n",
    "\n",
    "    new_row = pd.DataFrame({'PACK': [pack_number], 'num_SNPs': [len(df_files)]})\n",
    "    SNP_monitoring_df = pd.concat([SNP_monitoring_df, new_row], ignore_index=True)\n",
    "\n",
    "    ################ Eliminación de enlaces simbólicos del paquete anterior ################\n",
    "\n",
    "    files_to_remove = glob.glob(os.path.join(samples_directory, \"*_snps.all.ivar.tsv\"))\n",
    "\n",
    "    for file in files_to_remove:\n",
    "        if os.path.islink(file):\n",
    "            os.unlink(file)\n",
    "\n",
    "        # Verificación extra de que se han eliminado los enlaces simbólicos\n",
    "        elif os.path.exists(file):\n",
    "            os.remove(file)\n",
    "\n",
    "    print (f\"Los enlaces simbólicos del paquete {pack_number} han sido eliminados\")\n",
    "\n",
    "    ################ Nuevo paquete de 500 muestras ################\n",
    "\n",
    "    pack_number += 1\n",
    "    print (\"Inicio del procesamiento del siguiente paquete de muestras\")\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_df_files = f'/home/laura/andrea/scripts/TB_DDBB_Building/Filtrado_SNPs/Filtered_SNPs_{timestamp}.tsv'\n",
    "df_files.to_csv(output_df_files, sep='\\t', index=False)\n",
    "\n",
    "output_SNP_monitoring_df = f'/home/laura/andrea/scripts/TB_DDBB_Building/Curva_Rarefaccion/SNP_monitoring_curve_{timestamp}.tsv'\n",
    "SNP_monitoring_df.to_csv(output_SNP_monitoring_df, sep='\\t', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "andrea",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
